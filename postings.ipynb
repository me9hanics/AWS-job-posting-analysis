{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job posting gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The methods have been refactored and updated - this notebook is partially outdated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "import methods.scrape as sfuncs\n",
    "import methods.urls as urlfuncs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Website: karriere.at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import sites\n",
    "\n",
    "karriere_at = sites.KarriereATScraper()\n",
    "postings = karriere_at.gather_data(descriptions=True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of what we have gathered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of postings: 462\n",
      "Number of postings with description: 460\n",
      "Total number of characters: 1783171\n",
      "Average number of characters (excluding 0-length descriptions): 3876.458695652174\n",
      "Total number of words: 226275\n",
      "Average number of words (excluding 0-length descriptions): 491.9021739130435\n"
     ]
    }
   ],
   "source": [
    "num_postings = len(postings)\n",
    "num_postings_with_description = len([posting for posting in postings.values() if posting[\"description\"]])\n",
    "description_length_sum = np.sum([len(posting[\"description\"]) for posting in postings.values() if posting[\"description\"]])\n",
    "description_length_avg = np.mean([len(posting[\"description\"]) for posting in postings.values() if posting[\"description\"]])\n",
    "description_words_sum = np.sum([len(posting[\"description\"].split()) for posting in postings.values() if posting[\"description\"]])\n",
    "description_words_avg = np.mean([len(posting[\"description\"].split()) for posting in postings.values() if posting[\"description\"]])\n",
    "\n",
    "print(f\"Number of postings: {num_postings}\")\n",
    "print(f\"Number of postings with description: {num_postings_with_description}\")\n",
    "print(f\"Total number of characters: {description_length_sum}\")\n",
    "print(f\"Average number of characters (excluding 0-length descriptions): {description_length_avg}\")\n",
    "print(f\"Total number of words: {description_words_sum}\")\n",
    "print(f\"Average number of words (excluding 0-length descriptions): {description_words_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing the amount of characters we will load into AWS' services (e.g. Comprehend), we can calculate roughly the costs of using these models.<br>\n",
    "Amazon Comprehend pricing is quite standard: *\"NLP requests are measured in units of 100 characters, with a 3 unit (300 character) minimum charge per request\"*.<br>The pricing is as follows: *\"Key Phrase Extraction/Translation:\t$0.0001 per unit (under 10M units)\"*.\n",
    "\n",
    "Since there is a minimum charge that doesn't change until 300+ characters, we use 299 character long text sections for initially detecting text language, and this is also the reason to not use the `detect_dominant_language` method separately for each keyword.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are our expected costs? We can utilize the fact that on average, our job descriptions are 3876.46 characters long, i.e. 39 units.<br>\n",
    "Aside small-scale translation (which is cheap), the costs are made up of key phrase extractions; the approximate cost of that: average description length of 39 units * 460 job postings * $0.0001/unit ~ 1.80 USD for every run.\n",
    "\n",
    "\n",
    "To reduce costs, let's filter out less relevant postings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352, 462)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banned_words = [\"manager\", \"management\", \"professor\", \"team leader\", \"teamleader\", \"teamleiter\", \"team leiter\",\n",
    "                \"internship\", \"jurist\", \"lawyer\", \"audit\",\"legal\", \"advisor\", \"owner\", \"officer\", \"controller\"]\n",
    "banned_words += [\"praktikant\", \"praktikantin\", \"praktikum\", \"trainee\", \"intern\", \"senior\", \"administrator\",\n",
    "                 ]\n",
    "capital_banned_words = [\"SAP\", \"HR\"]\n",
    "postings_filtered = {key: value for key, value in postings.items()\n",
    "                     if not any(banned_word in value[\"title\"].lower() for banned_word in banned_words)}\n",
    "postings_filtered = {key: value for key, value in postings_filtered.items()\n",
    "                        if not any(banned_word in value[\"title\"] for banned_word in capital_banned_words)}\n",
    "len(postings_filtered), len(postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduced costs by more than 24% overall! The new costs are approximately 1.37 USD per run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Consultant Technology Strategy & Advisory (all genders)',\n",
       " 'IT-Consultant Artificial Intelligence (AI) (all genders)',\n",
       " 'Senior Data Engineer (m/f/d)',\n",
       " 'Conversational AI Specialist (all humans)',\n",
       " 'Assistant Professor with tenure track to establish a Research Group for Artificial Intelligence / Machine Learning in the Life Sciences',\n",
       " 'Games Analyst (f/m/d)',\n",
       " 'Senior Full Stack Data Scientist_in',\n",
       " 'Data Scientist_in',\n",
       " 'DevOps Engineer (w/m/x)',\n",
       " 'KI-Experte für Softwarelösungen (m/w/d) - AI Solution Architect']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = [posting[\"title\"] for posting in postings.values()]\n",
    "titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_today = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "with open(f\"source/save/postings_{date_today}.json\", \"w\") as f:\n",
    "    json.dump(postings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for analyzing job postings is stored in the `comprehend_keywords.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TieTalent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In works**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "url = 'https://tietalent.com/en/jobs?search=positions%5B0%5D%3DData_Engineer_5%26positions%5B1%5D%3DData_Analyst_36%26positions%5B2%5D%3DData_Scientist_37%26positions%5B3%5D%3DMachine_Learning_7%26positions%5B4%5D%3DBusiness_Intelligence_39%26positions%5B5%5D%3DNLP_14%26locations%5B0%5D%3DVienna_Vienna_Austria_304'\n",
    "response = session.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this website there is no scrolling needed, but checking page 2, 3, ... will be desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
